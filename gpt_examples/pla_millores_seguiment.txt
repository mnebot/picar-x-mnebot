PLA DE MILLORA PER AL SEGUIMENT DEL PICAR-X
============================================

Aquest document conté el pla d'implementació per al seguiment de persones
del picar-x seguint una estratègia alternativa partint de main.

TASQUES DE FINALITZACIÓ DE FASE (Aplicables a totes les fases)
---------------------------------------------------------------

Al final de cada fase, s'han de completar les següents tasques abans de considerar
la fase finalitzada. Aquestes tasques asseguren qualitat, mantenibilitat i
documentació del codi:

1. **Refactorització**
   - Aplicar principi DRY (Don't Repeat Yourself): eliminar codi duplicat
   - Eliminar classes, mètodes i línies innecessàries
   - Modularització: separar funcionalitats en mòduls lògics
   - Escriure tots els comentaris en català
   - Revisar noms de variables i funcions per claredat
   - Optimitzar imports i dependències
   - Aplicar principis SOLID quan sigui apropiat

2. **Validació de qualitat del codi i seguretat**
   - Revisar gestió d'errors i excepcions
   - Validar inputs i paràmetres
   - Revisar gestió de recursos (memòria, threads, sensors)
   - Verificar que no hi hagi memory leaks
   - Revisar seguretat de threads i locks
   - Validar que els sensors es gestionin correctament
   - Revisar gestió de fitxers i permisos

3. **Creació/actualització de test unitaris**
   - Crear tests per a les noves funcionalitats
   - Actualitzar tests existents si cal
   - Assegurar cobertura mínima del 70% per a codi nou
   - Tests han de ser independents i repetibles
   - Utilitzar mocks per a sensors i hardware quan sigui necessari

4. **Creació/actualització de bateria de tests d'inici**
   - Crear/actualitzar tests que s'executen quan el robot s'inicia
   - Tests de sensors (ultrasònic, càmera, etc.)
   - Tests de moviment bàsic (endavant, endarrere, gir)
   - Tests de càmera (pan/tilt)
   - Afegir opció per executar o no aquests tests des de gpt_car.py
   - Implementar flag --skip-tests o --run-tests

5. **Detecció de deute tècnic**
   - Crear/actualitzar fitxer deute_tecnic.txt
   - Documentar decisions tècniques preses per rapidesa
   - Documentar codi que necessita millores futures
   - Documentar limitacions conegudes
   - Documentar dependències problemàtiques
   - Prioritzar deute tècnic segons impacte i esforç

6. **Actualització del README**
   - Actualitzar README.md amb les noves funcionalitats implementades
   - Documentar canvis en l'arquitectura o estructura del codi
   - Afegir/explicar noves dependències si n'hi ha
   - Actualitzar instruccions d'ús si han canviat
   - Documentar noves funcionalitats i com utilitzar-les
   - Mantenir README actualitzat amb l'estat actual del projecte

**Nota:** Aquestes tasques s'apliquen al final de cada fase. Per evitar repetició,
aquestes tasques es referencien com "Tasques de finalització de fase" al final
de cada fase del pla.


FILOSOFIA DE L'ESTRATÈGIA
--------------------------

En lloc de seguir una persona movent el robot complet, aquesta estratègia es basa en:
1. **Seguiment visual pur amb càmera** (com stare_at_you.py)
2. **Moviment del robot només quan la persona surt del camp de visió**
3. **Aproximació gradual quan la persona està centrada**
4. **Estratègia de "mira i mou" en comptes de "segueix continuament"**

Aquesta estratègia és més simple, menys propensa a errors, i més fàcil de depurar.


FASES DE LA IMPLEMENTACIÓ
--------------------------

FASE 1: SEGUIMENT VISUAL PUR (Base)
------------------------------------

1.1. Implementar seguiment de càmera (pan/tilt) basat en stare_at_you.py
   - Utilitzar exactament la mateixa lògica que funciona
   - Centrar la persona a la imatge amb pan/tilt
   - No moure el robot encara

1.2. Afegir filtre de suavització per càmera
   - Mitjana mòbil de 3-5 deteccions
   - Limitar velocitat de canvi d'angle

1.3. Detectar quan la persona està "centrada"
   - Definir zona central (ex: ±30 píxels del centre)
   - Mantenir estat: "centrada" o "no centrada"

**Tasques de finalització de fase** (veure secció comuna al principi del pla)


FASE 2: MOVIMENT REACTIU (Quan surt del camp de visió)
------------------------------------------------------

2.1. Detectar quan la persona surt del camp de visió
   - Si no es detecta persona durant 0.5s
   - Recordar última posició coneguda (dreta/esquerra)
   - Girar el robot cap a la direcció on va la persona

2.2. Estratègia de recerca
   - Girar 30 graus cap a la direcció on va la persona
   - Aturar i buscar amb càmera
   - Si no es troba, girar una mica més
   - Repetir fins a trobar o timeout (5s)

2.3. Quan es troba de nou
   - Centrar amb càmera primer
   - Després moure cap a la persona si està massa lluny

**Tasques de finalització de fase** (veure secció comuna al principi del pla)


FASE 3: APROXIMACIÓ QUAN ESTÀ CENTRADA
---------------------------------------

3.1. Quan la persona està centrada a la imatge
   - Llegir distància amb sensor ultrasònic
   - Si distància > OPTIMAL_DISTANCE + 10cm: avançar lentament
   - Si distància < OPTIMAL_DISTANCE - 10cm: retrocedir lentament
   - Si està dins zona òptima: aturar

3.2. Velocitat adaptativa
   - Velocitat proporcional a l'error de distància
   - Màxim 30% de velocitat
   - Aturar si error < 5cm

**Tasques de finalització de fase** (veure secció comuna al principi del pla)


FASE 4: ESTATS I MÀQUINA D'ESTATS
----------------------------------

4.1. Definir estats del sistema
   - STATE_SEARCHING: Buscant persona (girant, movent càmera)
   - STATE_TRACKING: Seguint amb càmera (persona visible)
   - STATE_APPROACHING: Aproximant-se (persona centrada, ajustant distància)
   - STATE_LOST: Persona perduda (recerca activa)

4.2. Transicions d'estat
   - SEARCHING -> TRACKING: Persona detectada
   - TRACKING -> APPROACHING: Persona centrada durant 0.3s
   - APPROACHING -> TRACKING: Persona desviada
   - TRACKING -> LOST: Persona no detectada durant 0.5s
   - LOST -> SEARCHING: Iniciar recerca
   - LOST -> TRACKING: Persona trobada durant recerca

4.3. Comportament per estat
   - Cada estat té comportament clar i definit
   - Facilita debugging i millores

**Tasques de finalització de fase** (veure secció comuna al principi del pla)


FASE 5: MILLORES AVANÇADES (Opcional)
--------------------------------------

5.1. Predicció de moviment
   - Si la persona es mou consistentment en una direcció, predir on anirà
   - Començar a girar abans que surti del camp de visió

5.2. Evitació d'obstacles
   - Si sensor detecta obstacle mentre s'aproxima, aturar
   - Buscar camí alternatiu

5.3. Seguiment de múltiples persones
   - Si hi ha diverses persones, seguir la més propera o central

**Tasques de finalització de fase** (veure secció comuna al principi del pla)


ESTRUCTURA DEL CODI
-------------------

```python
# Estats
STATE_SEARCHING = 'searching'
STATE_TRACKING = 'tracking'
STATE_APPROACHING = 'approaching'
STATE_LOST = 'lost'

# Variables d'estat
current_state = STATE_SEARCHING
person_centered_time = 0
last_detection_time = 0
last_person_direction = None  # 'left' o 'right'

def follow_me_handler_v2():
    global current_state, person_centered_time, last_detection_time
    
    # Configuració
    CENTER_ZONE = 30  # píxels
    OPTIMAL_DISTANCE = 50  # cm
    DISTANCE_TOLERANCE = 10  # cm
    
    # Filtre per càmera
    cam_pan_angle = 0
    cam_tilt_angle = DEFAULT_HEAD_TILT
    detection_buffer = []
    
    while True:
        if not follow_me_active:
            time.sleep(0.1)
            continue
        
        try:
            human_n = Vilib.detect_obj_parameter.get('human_n', 0)
            
            if human_n != 0:
                last_detection_time = time.time()
                person_x = Vilib.detect_obj_parameter['human_x']
                person_y = Vilib.detect_obj_parameter['human_y']
                
                # Filtrar detecció
                detection_buffer.append((person_x, person_y))
                if len(detection_buffer) > 5:
                    detection_buffer.pop(0)
                person_x = sum(x for x, y in detection_buffer) / len(detection_buffer)
                person_y = sum(y for x, y in detection_buffer) / len(detection_buffer)
                
                # Calcular offset
                offset_x = person_x - CAMERA_CENTER_X
                offset_y = person_y - CAMERA_CENTER_Y
                
                # Seguiment amb càmera (igual que stare_at_you.py)
                if abs(offset_x) > 5:  # Zona morta petita
                    cam_pan_angle += (offset_x * 10 / 640) - 5
                    cam_pan_angle = clamp_number(cam_pan_angle, -35, 35)
                    my_car.set_cam_pan_angle(cam_pan_angle)
                
                if abs(offset_y) > 5:
                    cam_tilt_angle -= (offset_y * 10 / 480) - 5
                    cam_tilt_angle = clamp_number(cam_tilt_angle, -35, 35)
                    my_car.set_cam_tilt_angle(cam_tilt_angle)
                
                # Determinar si està centrada
                is_centered = abs(offset_x) < CENTER_ZONE and abs(offset_y) < CENTER_ZONE
                
                # Màquina d'estats
                if current_state == STATE_SEARCHING:
                    if is_centered:
                        current_state = STATE_APPROACHING
                        person_centered_time = time.time()
                    else:
                        current_state = STATE_TRACKING
                
                elif current_state == STATE_TRACKING:
                    if is_centered and (time.time() - person_centered_time) > 0.3:
                        current_state = STATE_APPROACHING
                        person_centered_time = time.time()
                    elif not is_centered:
                        person_centered_time = 0
                
                elif current_state == STATE_APPROACHING:
                    if not is_centered:
                        current_state = STATE_TRACKING
                        my_car.stop()
                    else:
                        # Ajustar distància
                        distance = my_car.get_distance()
                        distance_error = distance - OPTIMAL_DISTANCE
                        
                        if abs(distance_error) > DISTANCE_TOLERANCE:
                            if distance_error > 0:
                                # Massa lluny, avançar
                                speed = min(30, int(20 + distance_error * 0.5))
                                my_car.forward(speed)
                            else:
                                # Massa a prop, retrocedir
                                speed = min(30, int(20 + abs(distance_error) * 0.5))
                                my_car.backward(speed)
                        else:
                            my_car.stop()
                
                # Recordar direcció per si es perd
                if offset_x > 0:
                    last_person_direction = 'right'
                else:
                    last_person_direction = 'left'
            
            else:
                # No es detecta persona
                if current_state != STATE_LOST:
                    if time.time() - last_detection_time > 0.5:
                        current_state = STATE_LOST
                        my_car.stop()
                
                elif current_state == STATE_LOST:
                    # Recerca activa
                    if last_person_direction:
                        # Girar cap a la direcció on va la persona
                        if last_person_direction == 'right':
                            my_car.set_dir_servo_angle(20)
                            my_car.forward(20)
                        else:
                            my_car.set_dir_servo_angle(-20)
                            my_car.forward(20)
                        
                        time.sleep(0.3)
                        my_car.stop()
                        my_car.set_dir_servo_angle(0)
                    
                    # Si no es troba després de 3s, tornar a SEARCHING
                    if time.time() - last_detection_time > 3.0:
                        current_state = STATE_SEARCHING
                        last_person_direction = None
            
        except Exception as e:
            print(f'[Follow Me] Error: {e}')
            my_car.stop()
        
        time.sleep(0.05)
```
